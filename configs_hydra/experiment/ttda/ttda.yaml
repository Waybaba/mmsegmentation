# @package _global_

ttda_hook: 
  type: TTDAHook
  turn_on_adapt: true # for test source-only
  adapt_img_vis_freq: ??? # for adapt, can see the proto for learning
  use_pseudo_label: ???
  pseudo_use_ori: true # use freezed source label for pseudo label
  slide_adapt: ???
  high_conf_mask:
    turn_on: ???
    metric: confidence # TODO to implement
    use_history: true
    top_p: 0.5 # only keep top_p pixels, others set to 255
  proto_predict:
    turn_on: ???
    rho: count # for updating prototype, history_proto = (1-rho) * history_proto + rho * proto_this_img
    lambda_: 0.0 # for using prototype, final_proto = lambda_ * history_proto + (1 - lambda_) * proto_this_img. if "count", use count as lambda
    # norm_feats: true
    debug_feats: "prob" # feats, logits, prob
    norm_feats_mean: true
    norm_feats_sim: true
    proto_mask_top_p: 0.9 # 
    proto_mask_metric: "entropy" # confidence, entropy, ground_truth, none
    proto_mask_use_history: false # whether top_p threshold is based on all history metric or only this img 
  sam_predict:
    turn_on: ???
    tau: 1.0 # temperature for sam feats pseudo weights
    type: logits_mask_adjust # logits_mask_adjust, sam_feats_proto
    use_prob: true
    sam_ratio: 1.0 # 0.0 means no sam
    sam_ratio_mul_conf: true # whether add confidence to sam feats
    sam_ratio_conf_adjust: 1.0 # 0.0 all one, 1.0 linear, 100.0 all zero
  pseudo_label_loss:
    ratio: 0.0 # 0. or 1. to only adjust entropy loss
    conf_weight_tau: 1.0 # 0.0 all one, 1.0 linear, 100.0 all zero
    # mask_with: "ground_truth" # "pseudo_label" or "entropy" or "ground_truth"
    # mask_with_freeze: false
    # threshold: 0.9
    # target: "current" # "current" or "freeze"
    # class_balance: false
  entropy_loss:
    ratio: 0.0
    # mask_with: "entropy" # "pseudo_label", "entropy"
    # mask_with_freeze: false 
    # threshold: 0.1 # use as ratio if "class_balance" = true
    # class_balance: false
    tau: 0.0 # temperature for entropy loss # would increase the weight of confident ones
  sam_loss:
    ratio: 1.0
    strategy: "close_to_confident" # min_variance, close_to_confident
    confidence_type: "entropy" # "confidence", "entropy"
    confidence_selected_ratio: 0.1
  diverse_loss:
    ratio: 0.0
  ema:
    turn_on: false
    rho: 0.3 # how much would ema be updated each step


  debug:
    use_mmseg_pseudo_loss: ???



optim_wrapper:
  paramwise_cfg:
    custom_keys:
      .norm: # ! since we can not have . in name
        lr_mult: ${norm_lr_mult} # ! how to decide order?
      head:
        lr_mult: 0.0
      back:
        lr_mult: 0.0 # does not train backbone
      token_prompt:
        lr_mult: 0.0
      tpt_gates: 
        lr_mult: 0.0
      visual_prompt_module:
        lr_mult: 0.0
      fusion_conv:
        lr_mult: 0.0
norm_lr_mult: 1.0

custom_hooks:
  - ${ttda_hook} # ! change ttda_hook.xxx so that could be logged in wandb

default_hooks:
  visualization:
    draw: ??? # true
    interval: ??? # draw seg figure
  logger:
    interval: 10

train_cfg:
  val_interval: 100 # not used since this is TTDA

load_from: ??? # ! load from mmseg model


### TPT
model:
  type: EncoderDecoderWrapper
  backbone:
    type: MixVisionTransformerTPT
    tpt_cfg:
      turn_on: false
      num_tokens: 3
      weight_init_type: normal # zero, normal, kv_normal, q_normal
      mode: llama # llama, vpt # TODO
    vpt_cfg:
      turn_on: false 
      type: cnn # "padding" # padding, fixed_patch, random_patch, cnn
      prompt_size: 3
      res_add: 1
      weight_init_type: normal # "normal" 
# model:
#   test_cfg:
#     mode: slide

train: false
test: true
tags: [ttda_debug]
task_name: MMSEG